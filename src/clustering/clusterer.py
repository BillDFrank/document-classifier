import streamlit as st
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import OneHotEncoder
import os
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Directory paths using pathlib.Path for cross-platform compatibility
BASE_DIR = Path(__file__).resolve().parent.parent.parent
PARQUET_DIR = BASE_DIR / "data" / "processed"

def perform_clustering(df, n_clusters):
    """Performs K-Means clustering on the dataset."""
    try:
        if df.empty:
            logger.warning("Empty DataFrame provided for clustering")
            return None, None

        if 'combined_text' not in df.columns:
            logger.error("Required column 'combined_text' not found in DataFrame")
            return None, None

        if n_clusters <= 0:
            logger.error(f"Invalid number of clusters: {n_clusters}")
            return None, None

        # Feature Engineering: One-hot encode the combined_text
        enc = OneHotEncoder(handle_unknown="ignore")
        X = enc.fit_transform(df[['combined_text']]).toarray()

        # Apply K-Means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        df["cluster"] = kmeans.fit_predict(X)

        logger.info(f"Successfully performed clustering with {n_clusters} clusters")
        return df, kmeans
        
    except Exception as e:
        logger.error(f"Error during clustering: {e}")
        return None, None

def app():
    st.title("Clustering")
    st.write("Suggests clusters to facilitate classification.")

    st.sidebar.title("Settings")

    # List all .parquet files in the data/processed directory
    if not PARQUET_DIR.exists():
        st.error(f"Directory '{PARQUET_DIR}' not found. Generate embeddings first.")
        logger.error(f"Parquet directory not found: {PARQUET_DIR}")
        return

    parquet_files = [f for f in PARQUET_DIR.iterdir() if f.is_file() and f.suffix == ".parquet"]
    if not parquet_files:
        st.error(f"No Parquet files found in '{PARQUET_DIR}'. Generate embeddings first.")
        logger.warning(f"No parquet files found in {PARQUET_DIR}")
        return

    # Let the user select a Parquet file
    selected_parquet = st.sidebar.selectbox(
        "Select a Parquet file to cluster",
        [f.name for f in parquet_files],
        help="Choose a Parquet file generated by the Datasource app."
    )

    # Construct the full path to the selected Parquet file
    parquet_file = PARQUET_DIR / selected_parquet

    n_similares = st.sidebar.slider("Number of Similar Elements", 1, 30, 10)
    n_clusters = st.sidebar.slider("Number of Clusters", 2, 50, 10)
    include_labeled = st.sidebar.checkbox(
        "Include Items with Labels",
        value=False,
        help="If checked, items with existing labels will be included in the labeling interface."
    )
    submit_button = st.sidebar.button("Submit")

    # Load DataFrame from the selected Parquet file into session state if not already loaded
    if "df" not in st.session_state:
        if not parquet_file.exists():
            st.error(f"File '{parquet_file}' not found. Generate the embeddings first.")
            logger.error(f"Parquet file not found: {parquet_file}")
            return
        
        try:
            st.session_state.df = pd.read_parquet(parquet_file)
            logger.info(f"Successfully loaded {len(st.session_state.df)} records from {parquet_file}")
        except Exception as e:
            st.error(f"Error loading parquet file: {e}")
            logger.error(f"Failed to load parquet file {parquet_file}: {e}")
            return

    # Perform clustering when "Submit" is clicked
    if submit_button:
        df = st.session_state.df.copy()
        df, kmeans = perform_clustering(df, n_clusters)

        if df is not None:
            st.session_state.df = df
            # Initialize cluster index if not already set
            if 'cluster' not in st.session_state:
                st.session_state.cluster = 0
            # Display cluster distribution
            st.write("### Cluster Distribution")
            cluster_counts = df['cluster'].value_counts().sort_index().to_dict()
            st.write({f"Cluster {k+1}": v for k, v in cluster_counts.items()})
            st.success("Clustering completed successfully! Use the labeling interface below to update labels.")
        else:
            st.error("Clustering failed. Please check the data and try again.")
            return

    # If clustering has been performed, show the labeling interface
    if "cluster" in st.session_state.df.columns:
        df = st.session_state.df
        # Ensure the label column is of string type
        df['label'] = df['label'].astype(str)
        # Get distinct labels from the DataFrame
        distinct_labels = [x for x in df['label'].unique() if pd.notnull(x) and x.strip() != ""]
        distinct_labels.sort()

        # Replace NaN or empty labels with an empty string for consistency
        df['label'] = df['label'].fillna("")

        # Conditionally filter the DataFrame based on the toggle
        if not include_labeled:
            df_filtered = df[df['label'] == ""]
        else:
            df_filtered = df  # Include all items, labeled or not

        # Get the current cluster index
        if 'cluster' not in st.session_state:
            st.session_state.cluster = 0
        cluster = st.session_state.cluster

        # Display clustering parameters
        st.header("Parameters")
        st.write(f"Displaying up to {n_similares} similar elements")
        st.write(f"Cluster: {cluster+1} of {n_clusters}")
        labels_count = df_filtered.shape[0]
        empty_labels_count = df_filtered['label'].eq("").sum()
        st.write(f"Number of elements without a label: {empty_labels_count}/{labels_count}")

        # Navigation buttons for clusters
        next_back_cols = st.columns([1, 1, 2])
        with next_back_cols[0]:
            if st.button("BACK"):
                st.session_state.cluster = (cluster - 1) % n_clusters
        with next_back_cols[1]:
            if st.button("NEXT"):
                st.session_state.cluster = (cluster + 1) % n_clusters

        # Update cluster index after button clicks
        cluster = st.session_state.cluster

        # Select similar documents based on the toggle
        if not include_labeled:
            similar_docs = df_filtered[(df_filtered['cluster'] == cluster) & (df_filtered['label'] == "")]
        else:
            similar_docs = df_filtered[df_filtered['cluster'] == cluster]

        # Warn if the cluster is empty or has fewer documents than requested
        if similar_docs.empty:
            st.warning(f"Cluster {cluster+1} is empty. Try navigating to another cluster or adjust the number of clusters.")
        elif len(similar_docs) < n_similares:
            st.info(f"Cluster {cluster+1} has only {len(similar_docs)} documents, fewer than the requested {n_similares}.")

        similar_docs = similar_docs.head(n_similares)

        # Labeling interface
        st.header("Selection of Cluster Labels")

        # Use distinct labels from the DataFrame
        existing_labels = distinct_labels if distinct_labels else []

        label_cols = st.columns([2, 3, 2])
        with label_cols[0]:
            selected_label = st.selectbox("Select a label", existing_labels) if existing_labels else st.text_input("Enter a label")
        with label_cols[1]:
            new_label = st.text_input("Add New Label")
        with label_cols[2]:
            if st.button("Add Label"):
                if new_label and new_label not in existing_labels:
                    existing_labels.append(new_label)
                    st.success(f"Label '{new_label}' added successfully.")

        st.header("Similar Elements in the Cluster")

        selected_similars = []
        for idx, row in similar_docs.iterrows():
            # Display the id_doc, combined_text, and label (if available)
            label_display = f" (Label: {row['label']})" if row['label'] else ""
            display_text = f"[{row['id_doc']}] - {row['combined_text'][0:1000]}{label_display}"
            if st.checkbox(display_text, key=idx, value=True):
                selected_similars.append(idx)

        if st.button("LABEL"):
            if selected_similars:
                # Update labels in the in-memory DataFrame
                label_to_apply = selected_label if existing_labels else new_label
                df_copy = st.session_state.df.copy()  # Create a copy to avoid modifying the original
                
                # Update labels in the copy
                for idx in selected_similars:
                    if idx in df_copy.index:
                        df_copy.at[idx, 'label'] = label_to_apply

                try:
                    # Ensure we're not losing any documents by comparing row counts
                    original_count = len(st.session_state.df)
                    new_count = len(df_copy)
                    if new_count < original_count:
                        st.error(f"Error: Document count decreased from {original_count} to {new_count}. Aborting save.")
                        return

                    # Save the entire DataFrame to ensure no documents are lost
                    df_copy.to_parquet(parquet_file, index=False)
                    st.session_state.df = df_copy
                    st.success("Elements labeled successfully and file updated!")
                except Exception as e:
                    st.error(f"Error saving file: {e}")
            else:
                st.warning("No elements selected for labeling.")

if __name__ == "__main__":
    app()