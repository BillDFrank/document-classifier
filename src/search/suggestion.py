import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os

# Directories
PARQUET_DIR = os.path.join("data", "processed")
MODELS_DIR = os.path.join("models")

def load_model():
    """Load the pre-trained classifier from the models directory."""
    model_path = os.path.join(MODELS_DIR, st.session_state.selected_model)
    try:
        model = joblib.load(model_path)
        return model
    except Exception as e:
        st.error(f"Failed to load model from {model_path} with joblib: {e}")
        try:
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            st.warning("Loaded with pickle as a fallback. Ensure compatibility.")
            return model
        except Exception as e2:
            st.error(f"Failed to load model with pickle fallback: {e2}")
            return None

def suggest_labels(df, model, batch_size):
    """Suggest labels for unlabeled documents in batches, grouped by predicted label and confidence."""
    unlabeled_df = df[df['label'].str.strip() == ""].copy()
    if unlabeled_df.empty or model is None:
        return None

    if 'embedding' not in df.columns:
        st.error("The 'embedding' column is missing. Ensure embeddings are precomputed.")
        return None

    # Extract embeddings for prediction (only for unlabeled rows)
    X_unlabeled = np.array(unlabeled_df['embedding'].tolist())
    predictions = model.predict(X_unlabeled)
    probabilities = model.predict_proba(X_unlabeled)
    max_probabilities = np.max(probabilities, axis=1)  # Confidence as max probability

    # Map numeric predictions to string labels using the label mapping
    label_mapping = st.session_state.label_mapping
    predicted_labels = [label_mapping[p] for p in predictions]
    unlabeled_df['suggested_label'] = predicted_labels
    unlabeled_df['confidence'] = max_probabilities

    # Group by predicted label and sort by confidence within each group
    grouped_df = unlabeled_df.groupby('suggested_label', group_keys=False).apply(
        lambda x: x.sort_values('confidence', ascending=False)
    )

    # Split into batches based on grouped labels
    batches = []
    for label, group in grouped_df.groupby('suggested_label'):
        label_batches = [group[i:i + batch_size] for i in range(0, len(group), batch_size)]
        batches.extend(label_batches)

    return batches

def app():
    st.title("Category Suggestion")
    st.write("Predicts and suggests labels for unlabeled documents.")

    st.sidebar.title("Settings")

    # List all .parquet files in the data/processed directory
    if not os.path.exists(PARQUET_DIR):
        st.error(f"Directory '{PARQUET_DIR}' not found. Generate embeddings first.")
        return

    parquet_files = [f for f in os.listdir(PARQUET_DIR) if f.endswith(".parquet")]
    if not parquet_files:
        st.error(f"No Parquet files found in '{PARQUET_DIR}'. Generate embeddings first.")
        return

    # Dropdown for selecting Parquet file
    selected_parquet = st.sidebar.selectbox(
        "Select a Parquet file",
        parquet_files,
        help="Choose a Parquet file generated by the Datasource app."
    )
    parquet_file = os.path.join(PARQUET_DIR, selected_parquet)

    # List all .pkl model files in the models directory
    if not os.path.exists(MODELS_DIR):
        st.error(f"Directory '{MODELS_DIR}' not found. Train a model first.")
        return

    model_files = [f for f in os.listdir(MODELS_DIR) if f.endswith(".pkl")]
    if not model_files:
        st.error(f"No .pkl model files found in '{MODELS_DIR}'. Train a model first.")
        return

    # Dropdown for selecting classifier model
    selected_model = st.sidebar.selectbox(
        "Select a Classifier Model",
        model_files,
        help="Choose a trained model (.pkl) from the models directory."
    )

    # Sidebar inputs
    batch_size = st.sidebar.slider("Maximum Elements per Batch", 1, 30, 10)
    submit_button = st.sidebar.button("Submit")

    # Initialize session state
    if 'full_df' not in st.session_state or st.session_state.get('last_parquet_file') != parquet_file or st.session_state.get('last_model') != selected_model:
        # Load the Parquet file at the start or when the file/model changes
        full_df = pd.read_parquet(parquet_file)
        if 'combined_text' not in full_df.columns:
            st.error("The 'combined_text' column is missing from the Parquet file.")
            return
        full_df['combined_text'] = full_df['combined_text'].astype(str)
        if 'label' not in full_df.columns:
            full_df['label'] = ""
        full_df['label'] = full_df['label'].astype(str).fillna("")
        # Do not filter out blank combined_text rows to preserve all 23,730 rows
        if full_df.empty:
            st.error("No rows found in the Parquet file.")
            return
        st.session_state.full_df = full_df
        st.session_state.last_parquet_file = parquet_file
        st.session_state.selected_model = selected_model
        st.session_state.last_model = selected_model
        st.session_state.current_batch = 0
        st.session_state.batches = None
        st.session_state.label_mapping = None

    if 'current_batch' not in st.session_state:
        st.session_state.current_batch = 0
    if 'possible_labels' not in st.session_state:
        st.session_state.possible_labels = []
    if 'batches' not in st.session_state:
        st.session_state.batches = None
    if 'label_mapping' not in st.session_state:
        st.session_state.label_mapping = None

    # Load the model and predict labels when submitting
    if submit_button:
        model = load_model()
        if model is None:
            return

        # Create a mapping from numeric labels to string labels
        existing_labels = sorted([label for label in st.session_state.full_df['label'].unique() if label])
        model_classes = list(model.classes_)
        if len(model_classes) != len(existing_labels):
            st.error(f"Number of model classes ({len(model_classes)}) does not match number of unique labels ({len(existing_labels)}). Please ensure the model was trained with the correct labels.")
            return

        # Map numeric model classes to string labels (assumes order matches)
        label_mapping = {num_label: str_label for num_label, str_label in zip(model_classes, existing_labels)}
        st.session_state.label_mapping = label_mapping

        # Get possible labels (model classes mapped to strings + existing labels)
        mapped_classes = [label_mapping[num_label] for num_label in model_classes]
        possible_labels = sorted(list(set(mapped_classes + existing_labels)))
        st.session_state.possible_labels = possible_labels

        # Suggest labels and create batches (only for unlabeled rows)
        batches = suggest_labels(st.session_state.full_df, model, batch_size)
        st.session_state.batches = batches
        st.session_state.current_batch = 0

    # Display and labeling interface
    if st.session_state.batches is None or not st.session_state.batches:
        st.info("No unlabeled documents found or all documents are labeled.")
        return

    batches = st.session_state.batches
    total_batches = len(batches)
    current_batch = st.session_state.current_batch

    # Navigation buttons
    st.header("Batch Navigation")
    st.write(f"Batch: {current_batch + 1} of {total_batches}")
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        if st.button("Previous Batch"):
            st.session_state.current_batch = max(0, current_batch - 1)
    with col2:
        if st.button("Next Batch"):
            st.session_state.current_batch = min(total_batches - 1, current_batch + 1)

    # Update current batch
    current_batch = st.session_state.current_batch
    batch_df = batches[current_batch].copy()
    current_label = batch_df['suggested_label'].iloc[0]  # All documents in this batch have the same predicted label

    # Display current batch
    st.header(f"Batch {current_batch + 1} of {total_batches} (Predicted Label: {current_label})")
    st.write(f"Displaying up to {len(batch_df)} items sorted by confidence")

    # Display batch with manual label editing
    possible_labels = st.session_state.possible_labels
    for idx, row in batch_df.iterrows():
        col1, col2, col3 = st.columns([6, 2, 2])
        with col1:
            st.write(f"[{row['id_doc']}] - {row['combined_text'][:500]}...")
        with col2:
            suggested_label = row['suggested_label']
            new_label = st.selectbox(
                "Suggested Label",
                possible_labels,
                index=possible_labels.index(suggested_label) if suggested_label in possible_labels else 0,
                key=f"label_{idx}"
            )
        with col3:
            st.write(f"Confidence: {row['confidence']:.2f}" if pd.notnull(row['confidence']) else "Confidence: N/A")

        # Update the label in the batch DataFrame if changed
        if new_label != suggested_label:
            batch_df.at[idx, 'suggested_label'] = new_label

    if st.button("Save"):
        # Update the full DataFrame with the new labels from the current batch
        for idx in batch_df.index:
            st.session_state.full_df.at[idx, 'label'] = batch_df.at[idx, 'suggested_label']
        # Update the batch in session state
        batches[current_batch] = batch_df
        st.session_state.batches = batches

        # Save the full DataFrame to the Parquet file, preserving all rows
        try:
            st.session_state.full_df.to_parquet(parquet_file, index=False)
            st.success("Labels for the current batch updated successfully in the Parquet file!")
        except Exception as e:
            st.error(f"Error updating file: {e}")

    # Refresh suggestions after labeling
    if st.button("Refresh Suggestions"):
        model = load_model()
        if model is not None:
            batches = suggest_labels(st.session_state.full_df, model, batch_size)
            st.session_state.batches = batches
            st.session_state.current_batch = 0
            st.success("Suggestions refreshed!")

if __name__ == "__main__":
    app()